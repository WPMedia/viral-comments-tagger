{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader,Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "from sklearn.metrics import r2_score\n",
    "import time\n",
    "from IPython.display import HTML\n",
    "import json\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch.utils.data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, train_test_split, GroupKFold, GroupShuffleSplit\n",
    "\n",
    "batch_size = 1024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, hidden_dim, num_layers, dropout, bidirectional, num_classes, batch_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_dir = 2 if bidirectional else 1\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=self.in_dim, hidden_size=self.hidden_dim, num_layers=self.num_layers, dropout=self.dropout/2, bidirectional=self.bidirectional,\n",
    "                            batch_first=True)\n",
    "        self.gru = nn.GRU(self.hidden_dim * 2, self.hidden_dim, bidirectional=self.bidirectional, batch_first=True)\n",
    "\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(int(hidden_dim)*self.num_dir, 1),\n",
    "#             nn.Softmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        self.lstm.flatten_parameters()\n",
    "        self.gru.flatten_parameters()\n",
    "        \n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        gru_out, _ = self.gru(lstm_out)\n",
    "        \n",
    "        x = torch.cat((lstm_out, gru_out), 1)\n",
    "        y = self.fc(x)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_process(df: pd.DataFrame(), epoch_col: str, value_columns: list, y_axis_name: str, title: str):\n",
    "    \n",
    "    # code mostly based on: https://altair-viz.github.io/gallery/multiline_tooltip.html\n",
    "    plot_df = df.melt(id_vars=epoch_col, value_vars=value_columns, var_name='group', value_name=y_axis_name)\n",
    "    plot_df[y_axis_name] = plot_df[y_axis_name].round(4)\n",
    "    nearest = alt.selection(type='single', nearest=True, on='mouseover', fields=[epoch_col], empty='none')\n",
    "    line = alt.Chart().mark_line(interpolate='basis').encode(\n",
    "        x=f'{epoch_col}:Q',\n",
    "        y=f'{y_axis_name}:Q',\n",
    "        color='group:N',\n",
    "    ).properties(\n",
    "        title=title\n",
    "    )\n",
    "\n",
    "    # Transparent selectors across the chart. This is what tells us\n",
    "    # the x-value of the cursor\n",
    "    selectors = alt.Chart().mark_point().encode(\n",
    "        x=f'{epoch_col}:Q',\n",
    "        opacity=alt.value(0),\n",
    "    ).add_selection(\n",
    "        nearest\n",
    "    )\n",
    "\n",
    "    # Draw points on the line, and highlight based on selection\n",
    "    points = line.mark_point().encode(\n",
    "        opacity=alt.condition(nearest, alt.value(1), alt.value(0))\n",
    "    )\n",
    "\n",
    "    # Draw text labels near the points, and highlight based on selection\n",
    "    text = line.mark_text(align='left', dx=5, dy=-5).encode(\n",
    "        text=alt.condition(nearest, f'{y_axis_name}:Q', alt.value(' '))\n",
    "    )\n",
    "\n",
    "    # Draw a rule at the location of the selection\n",
    "    rules = alt.Chart().mark_rule(color='gray').encode(\n",
    "        x=f'{epoch_col}:Q',\n",
    "    ).transform_filter(\n",
    "        nearest\n",
    "    )\n",
    "\n",
    "    # Put the five layers into a chart and bind the data\n",
    "    return alt.layer(line, selectors, points, rules, text,\n",
    "              data=plot_df, width=600, height=300).interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(train_loader, val_loader, patience, model, criterion, optimizer, scheduler, verbose, plot_training):\n",
    "    valid_loss_min = np.Inf\n",
    "    patience = patience\n",
    "    # current number of epochs, where validation loss didn't increase\n",
    "    p = 0\n",
    "    # whether training should be stopped\n",
    "    stop = False\n",
    "\n",
    "    epochs = 20000\n",
    "    training_logs = []\n",
    "\n",
    "    for e in range(1, epochs + 1):\n",
    "        # print(time.ctime(), 'Epoch:', e)\n",
    "        train_loss = []\n",
    "        train_acc = []\n",
    "        model.train()\n",
    "        for batch_i, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.cuda(), target.float().cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            output = output[:,-1,:]\n",
    "            loss = criterion(output, target.unsqueeze(1))\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "            a = target.data.cpu().numpy()\n",
    "            b = output.squeeze().detach().cpu().numpy()\n",
    "            train_acc.append(r2_score(a, b))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        val_loss = []\n",
    "        val_acc = []\n",
    "        for batch_i, (data, target) in enumerate(val_loader):\n",
    "            data, target = data.cuda(), target.float().cuda()\n",
    "            output = model(data)\n",
    "            output = output[:,-1,:]\n",
    "            loss = criterion(output, target.unsqueeze(1))\n",
    "            val_loss.append(loss.item()) \n",
    "            a = target.data.cpu().numpy()\n",
    "            b = output.squeeze().detach().cpu().numpy()\n",
    "            val_acc.append(r2_score(a, b))\n",
    "\n",
    "        if e % 1 == 0 and verbose:\n",
    "            print(f'Epoch {e}, train loss: {np.mean(train_loss):.4f}, valid loss: {np.mean(val_loss):.4f}, train acc: {np.mean(train_acc):.4f}, valid acc: {np.mean(val_acc):.4f}')\n",
    "\n",
    "        training_logs.append([e, np.mean(train_loss), np.mean(val_loss), np.mean(train_acc), np.mean(val_acc)])\n",
    "\n",
    "        scheduler.step(np.mean(val_loss))\n",
    "\n",
    "        valid_loss = np.mean(val_loss)\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            # print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min, valid_loss))\n",
    "            torch.save(model.state_dict(), 'model.pt')\n",
    "            valid_loss_min = valid_loss\n",
    "            p = 0\n",
    "\n",
    "        # check if validation loss didn't improve\n",
    "        if valid_loss > valid_loss_min:\n",
    "            p += 1\n",
    "            # print(f'{p} epochs of increasing val loss')\n",
    "            if p > patience:\n",
    "                print('Stopping training')\n",
    "                stop = True\n",
    "                break        \n",
    "\n",
    "        if stop:\n",
    "            break\n",
    "\n",
    "    checkpoint = torch.load('model.pt')      \n",
    "    model.load_state_dict(checkpoint)\n",
    "    \n",
    "    if plot_training:\n",
    "        training_logs = pd.DataFrame(training_logs, columns=['Epoch', 'Train loss', 'Valid loss', 'Train accuracy', 'Validation accuracy'])\n",
    "        loss_plot = plot_training_process(df=training_logs, epoch_col='Epoch', value_columns=['Train loss', 'Valid loss'], y_axis_name='loss', title='Loss progress')\n",
    "        acc_plot = plot_training_process(df=training_logs, epoch_col='Epoch', value_columns=['Train accuracy', 'Validation accuracy'], y_axis_name='accuracy', title='Accuracy progress')\n",
    "        render(loss_plot & acc_plot)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model():\n",
    "    torch.manual_seed(42)\n",
    "    model = RNN(63, 256, 3, 0.1, True, 9, batch_size)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=.5)\n",
    "    model = nn.DataParallel(model.cuda())\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, patience=8, factor=0.5, verbose=True)\n",
    "    return model, criterion, optimizer, scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net_folds(X, X_test, y, folds, plot_training, batch_size, patience, verbose):\n",
    "\n",
    "    oof = np.zeros((len(X), 9))\n",
    "    prediction = np.zeros((len(X_test), 9))\n",
    "    scores = []\n",
    "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n",
    "        print('Fold', fold_n, 'started at', time.ctime())\n",
    "        X_train, X_valid = X[train_index], X[valid_index]\n",
    "        y_train, y_valid = y[train_index], y[valid_index]\n",
    "        \n",
    "        train_set = torch.utils.data.TensorDataset(torch.FloatTensor(X_train), torch.LongTensor(y_train))\n",
    "        val_set = torch.utils.data.TensorDataset(torch.FloatTensor(X_valid), torch.LongTensor(y_valid))\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size)\n",
    "                \n",
    "        model, criterion, optimizer, scheduler = initialize_model()\n",
    "        model = train_net(train_loader, val_loader, patience, model, criterion, optimizer, scheduler, verbose, plot_training)\n",
    "        \n",
    "        y_pred_valid = []\n",
    "        for batch_i, (data, target) in enumerate(val_loader):\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            p = model(data)\n",
    "            pred = p.cpu().detach().numpy()\n",
    "            y_pred_valid.extend(pred)\n",
    "            \n",
    "        y_pred = []\n",
    "        for i, data in enumerate(test):\n",
    "            p = model(torch.FloatTensor(data).unsqueeze(0).cuda())\n",
    "            y_pred.append(p.cpu().detach().numpy().flatten())\n",
    "            \n",
    "        oof[valid_index] = np.array(y_pred_valid)\n",
    "        scores.append(r2_score(y_valid, np.array(y_pred_valid)))\n",
    "\n",
    "        prediction += y_pred\n",
    "\n",
    "    prediction /= n_fold\n",
    "    \n",
    "    prediction = np.array(prediction).argmax(1)\n",
    "    \n",
    "    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n",
    "    print('--' * 50)\n",
    "    \n",
    "    return oof, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2007211, 20, 63)\n",
      "(2007211,)\n"
     ]
    }
   ],
   "source": [
    "data_x = np.load('data_x.npy')\n",
    "data_y = np.load('data_y.npy')\n",
    "print(data_x.shape)\n",
    "print(data_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_x, data_y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Wed Aug 21 14:41:36 2019\n",
      "Epoch 1, train loss: 0.0026, valid loss: 0.0026, train acc: -0.0333, valid acc: -0.0085\n",
      "Epoch 2, train loss: 0.0026, valid loss: 0.0025, train acc: -0.0030, valid acc: 0.0239\n",
      "Epoch 3, train loss: 0.0026, valid loss: 0.0025, train acc: -0.0105, valid acc: -0.0153\n",
      "Epoch 4, train loss: 0.0025, valid loss: 0.0024, train acc: 0.0098, valid acc: 0.0450\n",
      "Epoch 5, train loss: 0.0025, valid loss: 0.0024, train acc: 0.0126, valid acc: 0.0454\n",
      "Epoch 6, train loss: 0.0025, valid loss: 0.0024, train acc: 0.0285, valid acc: 0.0435\n",
      "Epoch 7, train loss: 0.0024, valid loss: 0.0024, train acc: 0.0335, valid acc: 0.0246\n",
      "Epoch 8, train loss: 0.0024, valid loss: 0.0024, train acc: 0.0352, valid acc: 0.0465\n",
      "Epoch 9, train loss: 0.0024, valid loss: 0.0024, train acc: 0.0371, valid acc: 0.0451\n",
      "Epoch 10, train loss: 0.0024, valid loss: 0.0024, train acc: 0.0392, valid acc: 0.0421\n",
      "Epoch 11, train loss: 0.0024, valid loss: 0.0024, train acc: 0.0382, valid acc: 0.0461\n",
      "Epoch 12, train loss: 0.0024, valid loss: 0.0024, train acc: 0.0411, valid acc: 0.0409\n",
      "Epoch 13, train loss: 0.0024, valid loss: 0.0024, train acc: 0.0426, valid acc: 0.0525\n",
      "Epoch 14, train loss: 0.0024, valid loss: 0.0024, train acc: 0.0427, valid acc: 0.0499\n",
      "Epoch 15, train loss: 0.0024, valid loss: 0.0024, train acc: 0.0437, valid acc: 0.0474\n",
      "Epoch 16, train loss: 0.0024, valid loss: 0.0024, train acc: 0.0450, valid acc: 0.0533\n",
      "Epoch 17, train loss: 0.0024, valid loss: 0.0024, train acc: 0.0399, valid acc: 0.0473\n",
      "Epoch 18, train loss: 0.0024, valid loss: 0.0024, train acc: 0.0476, valid acc: 0.0435\n",
      "Epoch 19, train loss: 0.0024, valid loss: 0.0024, train acc: 0.0446, valid acc: 0.0528\n",
      "Epoch 20, train loss: 0.0024, valid loss: 0.0024, train acc: 0.0448, valid acc: 0.0553\n",
      "Epoch 21, train loss: 0.0024, valid loss: 0.0024, train acc: 0.0460, valid acc: 0.0507\n",
      "Epoch 22, train loss: 0.0024, valid loss: 0.0024, train acc: 0.0480, valid acc: 0.0496\n",
      "Epoch 23, train loss: 0.0024, valid loss: 0.0024, train acc: 0.0474, valid acc: 0.0364\n",
      "Epoch 24, train loss: 0.0024, valid loss: 0.0024, train acc: 0.0460, valid acc: 0.0556\n",
      "Epoch 25, train loss: 0.0024, valid loss: 0.0024, train acc: 0.0442, valid acc: 0.0546\n",
      "Epoch 26, train loss: 0.0024, valid loss: 0.0026, train acc: 0.0485, valid acc: -0.0761\n",
      "Epoch 27, train loss: 0.0024, valid loss: 0.0024, train acc: 0.0490, valid acc: 0.0547\n",
      "Epoch 28, train loss: 0.0024, valid loss: 0.0024, train acc: 0.0513, valid acc: 0.0361\n",
      "Epoch 29, train loss: 0.0024, valid loss: 0.0024, train acc: 0.0459, valid acc: 0.0533\n",
      "Epoch 30, train loss: 0.0024, valid loss: 0.0024, train acc: 0.0490, valid acc: 0.0468\n",
      "Epoch 31, train loss: 0.0024, valid loss: 0.0024, train acc: 0.0477, valid acc: 0.0483\n",
      "Epoch 32, train loss: 0.0024, valid loss: 0.0024, train acc: 0.0500, valid acc: 0.0380\n",
      "Epoch 33, train loss: 0.0024, valid loss: 0.0024, train acc: 0.0538, valid acc: 0.0159\n",
      "Epoch 34, train loss: 0.0024, valid loss: 0.0024, train acc: 0.0557, valid acc: 0.0514\n",
      "Epoch    33: reducing learning rate of group 0 to 2.5000e-01.\n",
      "Epoch 35, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0582, valid acc: 0.0572\n",
      "Epoch 36, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0581, valid acc: 0.0570\n",
      "Epoch 37, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0560, valid acc: 0.0550\n",
      "Epoch 38, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0588, valid acc: 0.0572\n",
      "Epoch 39, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0553, valid acc: 0.0570\n",
      "Epoch 40, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0589, valid acc: 0.0570\n",
      "Epoch 41, train loss: 0.0024, valid loss: 0.0024, train acc: 0.0583, valid acc: 0.0316\n",
      "Epoch 42, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0572, valid acc: 0.0574\n",
      "Epoch 43, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0577, valid acc: 0.0569\n",
      "Epoch 44, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0582, valid acc: 0.0560\n",
      "Epoch 45, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0558, valid acc: 0.0578\n",
      "Epoch 46, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0574, valid acc: 0.0571\n",
      "Epoch 47, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0590, valid acc: 0.0575\n",
      "Epoch 48, train loss: 0.0024, valid loss: 0.0024, train acc: 0.0568, valid acc: 0.0546\n",
      "Epoch 49, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0604, valid acc: 0.0543\n",
      "Epoch 50, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0581, valid acc: 0.0576\n",
      "Epoch 51, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0603, valid acc: 0.0564\n",
      "Epoch 52, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0571, valid acc: 0.0578\n",
      "Epoch 53, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0591, valid acc: 0.0584\n",
      "Epoch 54, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0592, valid acc: 0.0571\n",
      "Epoch 55, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0567, valid acc: 0.0584\n",
      "Epoch 56, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0593, valid acc: 0.0587\n",
      "Epoch 57, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0574, valid acc: 0.0565\n",
      "Epoch 58, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0600, valid acc: 0.0554\n",
      "Epoch 59, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0587, valid acc: 0.0522\n",
      "Epoch 60, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0580, valid acc: 0.0575\n",
      "Epoch 61, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0597, valid acc: 0.0579\n",
      "Epoch 62, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0596, valid acc: 0.0587\n",
      "Epoch 63, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0569, valid acc: 0.0568\n",
      "Epoch 64, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0600, valid acc: 0.0578\n",
      "Epoch    63: reducing learning rate of group 0 to 1.2500e-01.\n",
      "Epoch 65, train loss: 0.0024, valid loss: 0.0024, train acc: 0.0616, valid acc: 0.0501\n",
      "Epoch 66, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0605, valid acc: 0.0589\n",
      "Epoch 67, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0621, valid acc: 0.0585\n",
      "Epoch 68, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0604, valid acc: 0.0582\n",
      "Epoch 69, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0571, valid acc: 0.0585\n",
      "Epoch 70, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0602, valid acc: 0.0585\n",
      "Epoch 71, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0586, valid acc: 0.0588\n",
      "Epoch 72, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0588, valid acc: 0.0571\n",
      "Epoch 73, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0611, valid acc: 0.0576\n",
      "Epoch 74, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0590, valid acc: 0.0582\n",
      "Epoch 75, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0590, valid acc: 0.0589\n",
      "Epoch 76, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0598, valid acc: 0.0589\n",
      "Epoch 77, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0574, valid acc: 0.0587\n",
      "Epoch 78, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0592, valid acc: 0.0582\n",
      "Epoch 79, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0586, valid acc: 0.0584\n",
      "Epoch 80, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0594, valid acc: 0.0586\n",
      "Epoch    79: reducing learning rate of group 0 to 6.2500e-02.\n",
      "Epoch 81, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0589, valid acc: 0.0590\n",
      "Epoch 82, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0610, valid acc: 0.0582\n",
      "Epoch 83, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0603, valid acc: 0.0582\n",
      "Epoch 84, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0595, valid acc: 0.0584\n",
      "Epoch 85, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0616, valid acc: 0.0587\n",
      "Epoch 86, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0588, valid acc: 0.0579\n",
      "Epoch 87, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0600, valid acc: 0.0591\n",
      "Epoch 88, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0602, valid acc: 0.0589\n",
      "Epoch 89, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0616, valid acc: 0.0579\n",
      "Epoch 90, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0615, valid acc: 0.0589\n",
      "Epoch 91, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0600, valid acc: 0.0570\n",
      "Epoch 92, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0630, valid acc: 0.0586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0607, valid acc: 0.0588\n",
      "Epoch 94, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0616, valid acc: 0.0588\n",
      "Epoch 95, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0590, valid acc: 0.0584\n",
      "Epoch 96, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0601, valid acc: 0.0589\n",
      "Epoch 97, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0606, valid acc: 0.0592\n",
      "Epoch 98, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0608, valid acc: 0.0591\n",
      "Epoch 99, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0607, valid acc: 0.0579\n",
      "Epoch 100, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0599, valid acc: 0.0590\n",
      "Epoch 101, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0612, valid acc: 0.0590\n",
      "Epoch 102, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0570, valid acc: 0.0592\n",
      "Epoch 103, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0610, valid acc: 0.0591\n",
      "Epoch   102: reducing learning rate of group 0 to 3.1250e-02.\n",
      "Epoch 104, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0584, valid acc: 0.0586\n",
      "Epoch 105, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0594, valid acc: 0.0589\n",
      "Epoch 106, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0590, valid acc: 0.0591\n",
      "Epoch 107, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0620, valid acc: 0.0592\n",
      "Epoch 108, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0591, valid acc: 0.0591\n",
      "Epoch 109, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0610, valid acc: 0.0592\n",
      "Epoch 110, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0603, valid acc: 0.0592\n",
      "Epoch 111, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0630, valid acc: 0.0589\n",
      "Epoch 112, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0593, valid acc: 0.0591\n",
      "Epoch 113, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0600, valid acc: 0.0592\n",
      "Epoch 114, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0603, valid acc: 0.0591\n",
      "Epoch 115, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0605, valid acc: 0.0589\n",
      "Epoch 116, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0624, valid acc: 0.0592\n",
      "Epoch 117, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0608, valid acc: 0.0591\n",
      "Epoch   116: reducing learning rate of group 0 to 1.5625e-02.\n",
      "Epoch 118, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0591, valid acc: 0.0591\n",
      "Epoch 119, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0624, valid acc: 0.0589\n",
      "Epoch 120, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0596, valid acc: 0.0591\n",
      "Epoch 121, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0594, valid acc: 0.0588\n",
      "Epoch 122, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0626, valid acc: 0.0593\n",
      "Epoch 123, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0597, valid acc: 0.0591\n",
      "Epoch 124, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0603, valid acc: 0.0589\n",
      "Epoch 125, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0605, valid acc: 0.0591\n",
      "Epoch 126, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0609, valid acc: 0.0590\n",
      "Epoch   125: reducing learning rate of group 0 to 7.8125e-03.\n",
      "Epoch 127, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0602, valid acc: 0.0592\n",
      "Epoch 128, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0595, valid acc: 0.0592\n",
      "Epoch 129, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0617, valid acc: 0.0591\n",
      "Epoch 130, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0595, valid acc: 0.0591\n",
      "Epoch 131, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0584, valid acc: 0.0592\n",
      "Epoch 132, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0594, valid acc: 0.0592\n",
      "Epoch 133, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0606, valid acc: 0.0593\n",
      "Epoch 134, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0624, valid acc: 0.0592\n",
      "Epoch 135, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0612, valid acc: 0.0592\n",
      "Epoch   134: reducing learning rate of group 0 to 3.9062e-03.\n",
      "Epoch 136, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0621, valid acc: 0.0589\n",
      "Epoch 137, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0630, valid acc: 0.0589\n",
      "Epoch 138, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0598, valid acc: 0.0592\n",
      "Epoch 139, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0610, valid acc: 0.0592\n",
      "Epoch 140, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0597, valid acc: 0.0591\n",
      "Epoch 141, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0592, valid acc: 0.0593\n",
      "Epoch 142, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0601, valid acc: 0.0591\n",
      "Epoch 143, train loss: 0.0024, valid loss: 0.0023, train acc: 0.0615, valid acc: 0.0592\n"
     ]
    }
   ],
   "source": [
    "n_fold = 5\n",
    "folds = KFold(n_splits=n_fold, shuffle=True, random_state=42)\n",
    "oof, prediction = train_net_folds(X_train, X_test, y_train, folds, True, batch_size, 40, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
